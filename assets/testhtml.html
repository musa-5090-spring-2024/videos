<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>MUSA 509 Spring 2024 Week 10</title>
</head>
<body>
<h1>MUSA 509 Spring 2024 Week 10</h1>
<p><span style="color:#808080">[00:00:00]</span> All right, so there's one more topic that I want to talk about, and it uses a lot of the different concepts that we've already gone through, but ties them together to allow you to do new things. And I think this is going to be useful both for a few people's final projects as well as for, other work that you might want to do going forward.</p>
<p><span style="color:#808080">[00:00:20]</span> By and large, Cloud Functions can get you pretty far as long as you're using JavaScript or Python. You can do most of what you need to do in Python or JavaScript in Cloud Functions and you don't have to worry about building your own containers.</p>
<p><span style="color:#808080">[00:00:37]</span> However, for some things You will need to build your own containers, particularly, for example, if in Python you're using some of the geospatial libraries that rely on GDAL or GEOS or any of the other open source geospatial tools, you may have to build your own container with those libraries installed in order to run a job using those libraries.</p>
<p><span style="color:#808080">[00:01:01]</span> Or, if you want to use a language that is not supported by Cloud Functions, such as R, or if you want to run a shell script with a tool like ogr2ogr, any of these kinds of cases, you'll have to build your own container to run your job in.</p>
<p><span style="color:#808080">[00:01:23]</span> Now so we're going to talk about how to build your own containers, but we're also going to talk about APIs a little bit. So in this class, we've used APIs to download data from various sources. But we haven't really gone over how to build APIs explicitly.</p>
<p><span style="color:#808080">[00:01:43]</span> It turns out you've been building APIs all this time by writing Cloud Functions. A Cloud Function is essentially an API. And we haven't been concerned about the return values from those APIs the outputs, the HTTP responses from those APIs. But today we're going to look at how we can be concerned about those in order to build an API that's more dynamic than the ones that we've been building.</p>
<p><span style="color:#808080">[00:02:17]</span> So, let's get into it. We'll start with talking about, before getting into creating a job in Cloud Run with your own container or creating an API, First, we're just going to talk about how you can work with some of the data that you've put into BigQuery from a script. And we've hinted at this before, but we haven't gone over it explicitly, so I'm going to go over it explicitly real quick.</p>

<h2>Working with BigQuery query results in a script</h2>
<p><span style="color:#808080">[00:02:43]</span> All right. So what we're going to do is we're going to use BigQuery client libraries to run a query in BigQuery. And then we are going to use the results from that query to do something like write a table or store a new file into cloud storage.</p>
<p><span style="color:#808080">[00:03:03]</span> So I have a new folder here and I'm going to go ahead and create a couple of new files. And I already created a requirements file and a package. json file but I'm going to come go ahead and move these into my folder here. We're going to treat this as if it's going to be a Google, a cloud function. And so I'll have to have a package. json or a requirements. txt file packaged within that cloud function just like we've done elsewhere.</p>
<p><span style="color:#808080">[00:04:22]</span> I'll move my node modules in there as well because I'll need that for running my node script. And in my package. json, I'll also specify that my main file is index. mjs.</p>
<p><span style="color:#808080">[00:05:01]</span> Great.</p>
<p><span style="color:#808080">[00:05:15]</span> Now I'll start off by importing env just to get my keys and anything else that I might need to run this script against BigQuery imported into my script.</p>
<p><span style="color:#808080">[00:05:43]</span> And just for your reference, I already have a env file over here and it has my Google application credentials key which is referring to this file here. and if I recall correctly, in Node, I have to include an additional module in order to get the env package to look in parent folders of my current folder. for my env file. So I'm going to pull open one of the previous functions that we had just so that I can remember how to do that because, I don't actually remember how to do all of these things every time I do them. I regularly refer to my past scripts where I've done one thing or another to remind myself of how I do those things.</p>
<p><span style="color:#808080">[00:07:51]</span> That looks fine, I'll leave that as is.</p>
<p><span style="color:#808080">[00:07:56]</span> This is what I want, the findconfig module.</p>
<p><span style="color:#808080">[00:08:09]</span> Now, since I want to use find config, that means I'll have to install it.</p>
<p><span style="color:#808080">[00:08:27]</span> Great.</p>
<p><span style="color:#808080">[00:08:30]</span> Now, I know that I'm also going to be using the BigQuery client library, so I'm going to go ahead and install them as well. BigQuery, I think. Update my requirements file. And I'm also going to run Google Cloud Storage later on. Install those for Node. </p>
<p><span style="color:#808080">[00:09:39]</span> Let's go ahead and import some of those libraries. And we'll create a BigQuery client. I like to call mine client when I create it.</p>
<p><span style="color:#808080">[00:11:02]</span> Now, I have a query that I want to run within BigQuery. I'm just going to paste that query in here.</p>
<p><span style="color:#808080">[00:12:12]</span> Now, I've already tested this query in the BigQuery console. I already know that it does what I want it to do. But., What it does is it will give me all of the OPA property entries joined with all of the PWD parcel entries where the parcel number is equal to the BRTID. </p>
<p><span style="color:#808080">[00:12:39]</span> Now I am converting both of those to a string. I'm making sure that they're both strings. The parcel number is already a string but the BRTID I'm casting to a string because it happens to be a number in my data set. which is off. I would recommend keeping IDs as strings always, but for whatever reason in the data set that I have, the BRT ID is a numeric type, so I'm casting it to a string, and then I'm padding both of those strings so that they're both 10 digits and inserting zeros on the left side if there's any less than 10 digits within the value. That's what this L pad or left pad does. Now that's, I'm just being extra careful to make sure that both of those strings match up between the two different data sets. So this level of care is usually not necessary, but I'm doing it just to be careful. </p>
<p><span style="color:#808080">[00:13:41]</span> So, I'm matching between the properties and the parcels on parcel number and BRT ID. I am filtering the data set for just properties in zip code 19104 because that's what I want to do. And I am pulling back the ID of the parcel, the last sale date. Now I am right here choosing the left 10 characters within the last sale date value. And that's basically just making sure that I only keep the year month day value within the date. If you count yyyy dd, this is 1, characters always. So, I'm keeping the left 10 characters of the last sale date. I'm also grabbing the last sale price. And I am grabbing the polygon from the Parcels dataset, the PWD Parcels dataset, and calling that Geometry. Alright, so, my resulting relation, my resulting table will have four columns defined as such.</p>
<p><span style="color:#808080">[00:14:53]</span> And I'm doing that both in the Python and the JavaScript. Now to run this query, to get my results, I can do something like this.</p>
<p><span style="color:#808080">[00:15:56]</span> Essentially what I'm going to get back in each one of these cases is a set of rows that I can iterate over. So let's take a look at that now. And I want to take a look at it by setting a breakpoint on, within these scripts. And then inspecting what we get back from them. </p>
<p><span style="color:#808080">[00:16:16]</span> So let's say const let's just say breakpoint. Nope, in javascript it is debugger. But that's okay. Console. log this line. I'm going to set a breakpoint on that line. I'm going to do the same over here.</p>
<p><span style="color:#808080">[00:16:56]</span> Great. Now let's run one of these. We'll go ahead and run the python first. It does take a bit for the actual query to execute.</p>
<p><span style="color:#808080">[00:17:50]</span> Alright, so, now we are here and we can see that the query results object is actually a row iterator. Now, what does that mean? Really, it just means that it's a special object that we can use to retrieve the results from BigQuery. So for example, let's see what we can do with this row iterator. I'm going to take it and treat it as if instead of a row iterator, it's a list.</p>
<p><span style="color:#808080">[00:18:14]</span> Let's say something like rows equals list of query results. Ah, and we can see that the iterator has already started. That means that we can't do what I wanted to do just now and that's because we're in the debugger has already inspected the list. So instead, I am going to stop this and let's say rows equals list</p>
<p><span style="color:#808080">[00:18:59]</span> Let's say that. Now, the query results object is probably actually a little bit more efficient Than converting it directly to a list Because it will likely allow us to stream the data from BigQuery Instead of downloading it all in one batch But for now We're just going to download it all in one batch.</p>
<p><span style="color:#808080">[00:19:17]</span> We're also going to output some additional helpful text just that we can see where we are within this script.</p>
<p><span style="color:#808080">[00:20:07]</span> Alright, let's check out this Python file again.</p>

<h2>BigQuery remembers (&quot;caches&quot;) query results</h2>
<p><span style="color:#808080">[00:20:18]</span> Now that time my query came back much faster. And this is something we haven't really talked about with BigQuery. But, when you run a specific query against BigQuery. What BigQuery will do is it will actually remember the results of that query. It'll store it in essentially a temporary table. And the next time you run that exact same query, it has to be the exact same query, but the next time you run that exact same query, it's going to actually grab the results from that temporary table in memory that it stored last time you ran the query.</p>
<p><span style="color:#808080">[00:20:57]</span> So, often times you'll find that when you run a query once and then you run that exact same query again, the second time it's going to be much faster and that's what happened here. Alright, so now let's take a look at these rows and we can see that this list here that we created, this rows list, is now )a long list of 11, 000, that's all of the parcels within the 19104 zip code. So 11, 655 of these row objects. Now each one of these row objects is not exactly a dictionary but like a dictionary. Let's take a look at one of them here.</p>
<p><span style="color:#808080">[00:21:45]</span> So this is one of those rows objects. So we can store it in a variable. We'll just get the first row. Rows zero. So now we can say row dot and see what we can do with this. We can convert it to a dictionary. So there you have all of the fields on this particular row. There's an ID field, a last sale date, a last sale price. Apparently I'm pulling the sale price down as a string. I probably would want to change that by casting this to a number. But I'm going to leave it the same for now, just because this is for demonstration purposes. And then I also have the geometry converted to a GeoJSON string as I requested. All right, great. </p>
<p><span style="color:#808080">[00:22:41]</span> Now, using these results, I can do anything that I want with them. I can iterate over them. I can add additional data in. I can aggregate them in some way. I can save them to a file. So, for example, in the in class project, one of the tasks is to take some results from BigQuery and store them in a GeoJSON file. If we wanted to do that, it could look something like this. </p>
<p><span style="color:#808080">[00:23:13]</span> So first of all, I'm going to import JSON because I'll need that, Python, and then for each row and rows. Well, first of all, let's create a feature collection. Features is equal to an empty list and for row and rows. Turns out that copilot mostly knows what I want to do here. But I'm going to type it out anyway just so that you can, so it can talk through it piece by piece. </p>
<p><span style="color:#808080">[00:23:48]</span> So I'm going to create a feature. That feature is going to contain, first of all, the type of And this is a GeoJSON object, so it needs to have a type. The type is going to be feature. It's going to have some properties. And I'm going to pull those properties from the properties that I downloaded in this particular query. So that's going to be id, lastSaleDate, and lastSalePrice. I'm not going to type that out since Copilot's got me there. And then also I want a geometry on my feature. And that geometry... So recall that I am pulling down the geometries as a GeoJSON string, but in order to include that GeoJSON string in a GeoJSON object, I have to parse that string out to actual objects. So I'm going to say Jason loads row geometry.</p>
<p><span style="color:#808080">[00:24:58]</span> Now I'm going to append that feature into my features list. And at the end of all of that, I'm going to construct my GeoJSON. In fact, it's not really GeoJSON yet. I'm not going to call it that because a GeoJSON is a particular formatted string. This is just a feature collection represented as a dictionary, so I'm going to call it FeatureCollection.</p>
<p><span style="color:#808080">[00:25:22]</span> It's going to have a type of FeatureCollection. And I'm done. And the features is going to be the features list that I created before. Now I can convert this feature collection to an actual GeoJSON string. I'm going to move this finished query line up to where the query is so it's actually true. And then with this GeoJSON string I can do whatever I want.</p>
<p><span style="color:#808080">[00:25:56]</span> I can take it, I can, uh, for example, upload it to Google Cloud Storage by creating a storage client and initializing a bucket and so on and so forth.</p>
<p><span style="color:#808080">[00:26:10]</span> Great, thanks Copilot. </p>
<p><span style="color:#808080">[00:26:18]</span> So, just as a recap, here we've run this query. We created a feature collection using the rows within the query. And then We dumped that feature collection out to a GeoJSON string and stored that string in Google Cloud Storage. We can do the same thing in JavaScript. So let's take a look at what we get back in query results from the node based bigQuery client.</p>
<p><span style="color:#808080">[00:27:33]</span> Great, so now here we are. We are at our breakpoint and we have our query results. And we can see here the query results is a little bit simpler than the query results from in Python where it was a row iterator object. In JavaScript that query results, it's just an array. A regular array just like any other array that we want.</p>
<p><span style="color:#808080">[00:27:56]</span> And well, let me qualify that. It's actually an array with one thing that is a regular array of a whole bunch of stuff that we want.</p>
<p><span style="color:#808080">[00:28:09]</span> So we can take a look at each one of these. Each one of them is just a JavaScript object that we can use. So in other words, to achieve the same thing in JavaScript that we did in Python, we would say something like this, cons rows equals query results zero. And now we're finished with the query. And then, features, we can create an empty array.</p>
<p><span style="color:#808080">[00:28:45]</span> And it probably knows what I want to do, so I'm not going to type it all out this time because we already saw it once in Python. So essentially it's going to push on a new feature into our array of features as we did in Python. In fact, I like how it does it there without creating a whole other object, so I'm just going to copy that for the Python.</p>
<p><span style="color:#808080">[00:29:16]</span> So these are functionally equivalent now. And then, down below, we can say const featureCollection is equal to great. And, by the way, this is just creating a JavaScript object with an attribute type with a value of featureCollection and then with an attribute features with the value of our features array.</p>
<p><span style="color:#808080">[00:29:44]</span> This is just a shorthand. So if the name of the variable that you're storing in an attribute and an object is the same as the name of the key that you want to store that variable in, then you can just put the name of the variable. This is equivalent to features, or for that matter, in JavaScript, the quotes aren't necessary.</p>
<p><span style="color:#808080">[00:30:07]</span> So this video. In JavaScript, you can just leave off that key name since it's the name of the variable, or the same as the name of the variable. Finally, we can create a GeoJSON string by stringifying our feature collection. We don't actually need those other arguments. And then we can log uploading. to GCS, just for parity. And we can pull the bucket name from the environment variable, but I'm just demonstrating how to do this. I don't actually have a bucket named my bucket. If I did, then I could upload this, but this is just as a demonstration.</p>
<p><span style="color:#808080">[00:31:10]</span> Great! That really is proper use. Alright, so these two files are right now equivalent. I'm going to stop my node debugger here and remove my breakpoints as well. </p>
<p><span style="color:#808080">[00:31:27]</span> Now, these aren't cloud functions yet. Of course, I would need to import first I would have to install functions framework in each case and then import functions framework. And then define my function and tab all of this in and make sure that I return some return value. And then I could convert this one to a function as well. But, that is the idea of how you can pull down data from a BigQuery query, work with it, and do whatever you want with it.</p>
<p><span style="color:#808080">[00:32:20]</span> If I wanted, I could have I've manipulated the rows of this table, or this query, and then wrote those rows back into BigQuery in a different table. And that can be useful for when there is a, an operation that you want to do on your data SQL. So for example, I'm, Working with WMATA down in DC right now and one of the things that they end up doing with some of their data is inferring the gender of their riders based on the name of the accounts on their their tap cards.</p>
<p><span style="color:#808080">[00:33:07]</span> Questionable maybe, but that's what they do. , but that's hard to do in sql and they actually use a Python library to do that operation. And so what they end up doing is pulling the data out of the warehouse, running a script over the rows in Python, and then writing the resulting data back into the warehouse so that they think they can use it for analytical queries down the line.</p>
<p><span style="color:#808080">[00:33:32]</span> All of that is possible using these methods. Alright,</p>
<p><span style="color:#808080">[00:33:46]</span> so one quick note there is a pandas extension library called pandas gbq that you can use if you're using Python, and with pandas gbq, which stands for Google BigQuery, gbq, you can essentially pull a query results directly into a pandas data frame, or for that matter, Write a pandas data frame directly into a BigQuery table.</p>
<p><span style="color:#808080">[00:34:17]</span> It's pretty handy But it's also unnecessary as we just demonstrated over here </p>

<h2>Creating a job in Cloud Run from a container image</h2>
<p><span style="color:#808080">[00:34:26]</span> All right. So now let's talk about creating a job in Cloud Run. Now in each one of the project Repositories for the in class project. There is a story an issue about creating a task to update vector tiles for the current assessment values. And this task recommends that you use something called Cloud Run in order to run that task. </p>
<p><span style="color:#808080">[00:34:54]</span> Now, what is Cloud Run? Just reading from here, Google Cloud Run allows you to specify instructions for installing your dependencies and running your program in a virtual machine. on Google's infrastructure. A virtual machine into which your dependencies is pre installed is called a container. So when I say a container, essentially what I mean is just a, a virtual computer that has the dependencies that you need to run whatever you want to run in it installed. So you can use these containers to create new machines within Google Cloud and then run them. Now this is essentially what Google Cloud is doing when you create a new cloud function. It's just that Google knows how to create containers for things like Python and things like Node.</p>
<p><span style="color:#808080">[00:35:50]</span> Now in order to create a container you essentially have to define how to install all of the dependencies that go into that container and then how to start whatever should be running when the container starts. So one of the common ways to do that is with something called a dockerfile. And docker is a platform for creating and managing containers.</p>
<p><span style="color:#808080">[00:36:17]</span> There are other platforms but docker is one of the most commonly used ones. So this down here is an example of a dockerfile which you can use to define how to create a container that can run ogr2ogr. So this Dockerfile uses a, is based on a different container image called Cloud SDK. So Google Cloud, as well as a number of other companies and organizations, publish containers that you can build on. In fact, if you go to Docker Hub. You can find a ton of different containers for pretty much anything you can think of. There are containers for running Postgres where you can just download this Docker container and spin up a Postgres wherever you want, as long as Docker is installed on that machine. There are dock, there are docker containers for Redis. There are docker containers for airflow. If you recall. Lori talked about using Airflow. In fact, we want Apache Airflow.</p>
<p><span style="color:#808080">[00:37:45]</span> There is an official Airflow container. This one, Apache Airflow. Lori talked about using Airflow in the CalITP work that they were doing. And I've also mentioned it in past lectures. It is essentially an orchestrator. That you can use like we're using Google Cloud Workflows. But you can download a, an, a Docker container, a Docker file, a Docker image for Airflow, and essentially get started quickly with a basic Airflow setup.</p>
<p><span style="color:#808080">[00:38:24]</span> Now, you can base your Docker images off of one of these other publicly available images. And so essentially that is what we're doing. That is what we're doing here, where Google Cloud publishes their own Docker images. And so I'm using this one called Cloud SDK that allows me to use things it allows us to use tools that Google Cloud publishes. </p>
<p><span style="color:#808080">[00:39:01]</span> So, creating a docker image from, and this syntax here is specific to creating docker images. It has its own syntax, a docker file has its own syntax, and you can find information about docker file syntax readily available, published by docker documentation, docker docs. But these are all of the different keywords that you can use within a docker file, one of which is from. So this from keyword creates a new build stage from a base image. So in this case, this is my base image and I'm creating a new image from that base image.</p>
<p><span style="color:#808080">[00:39:43]</span> Run is exactly what it sounds like. So it starts with this base image and then inside of that base image, it will run this command. apt get is just a command that you can use to install things on Linux computers. And this cloud SDK image is a Linux Image. So it's updating its repository of stuff that can be installed, and then it's installing GDAL.</p>
<p><span style="color:#808080">[00:40:12]</span> So run agate install GAL. It's setting the working directory to a folder called Workspace which is a folder that exists inside of this image, this base image by default. And then it's copying over some files from wherever you run this or you build this docker image. In particular, there is a file that I'm suggesting you create called script. sh, which is a shell script file. And then it's going to copy that into the workspace folder. After that, it's going to make the script. sh file runnable. And it's going to start that script. sh file. So the cmd keyword specifies the default command for the container that you're creating.</p>
<p><span style="color:#808080">[00:41:04]</span> So after all of that here is the definition of the script. sh shell script. And in here, this is simply some preamble in order to, output any errors that happen to the terminal so that you see them and that you can debug them easily. Now, in the actual script we are copying the property tile. geo. json file. So this is, for example, a file that would be created by a function like the one that we just saw a few minutes ago.</p>
<p><span style="color:#808080">[00:41:40]</span> So it's copying that file from Google Cloud Storage into this image. Into this virtual machine, essentially. And then it's creating a file called PropertyTile. info from that file that it's copying down from Google Cloud Storage. Next, it's running ogr2ogr, and just to remind you, ogr2ogr is a program that you can use to convert from any number of geospatial file formats into any number of other geospatial file formats.</p>
<p><span style="color:#808080">[00:42:16]</span> In this particular case, we are taking as an input that property tile info. geo. json file that was just downloaded in the previous command. And we are generating Mapbox Vector Tiles, or MVTs, from it. Now, this doesn't actually have anything to do with Mapbox, except that the Mapbox Vector Tile standard was developed initially by Mapbox.</p>
<p><span style="color:#808080">[00:42:42]</span> But, it is an open standard that anyone can use, regardless of whether they're using Mapbox within their project or not. So, we are generating MVT tiles from Zoom Level 12 to Zoom Level 18 with no compression and we're sticking it into a folder called Properties. And then as the last step in this script, we're taking that Properties folder, copying it up to Google Cloud Storage into a bucket of our choosing. And in fact, if we go and check out one of the folders from last year, you can see what these what this results in.</p>
<p><span style="color:#808080">[00:44:03]</span> So, this is actually in this case, they generate tiles from zoom level 12 to zoom level 20, instead of just to zoom level 18. So, But inside of each one of these, you can see a Z, X, Y arrangement of files. Where Z is equal to 12, that's the zoom level, and then you have an X of, for example, 1191, and a Y of 1549.</p>
<p><span style="color:#808080">[00:44:30]</span> Each one of these files is a protobuf file. That's simply the format of file that Mapbox Vector Tiles uses in order to store the vector data.</p>
<p><span style="color:#808080">[00:44:48]</span> So, after you have a Cloud Run container, and in fact, let's go ahead and see what this looks like in an actual folder.</p>
<p><span style="color:#808080">[00:45:34]</span> By the way, a shell script doesn't have to be named script, it can be named anything. You can name it main, you can name it index, you can name it do my stuff dot sh. I just chose to name it script.</p>
<p><span style="color:#808080">[00:45:55]</span> Now if I were doing this for real, I would want to actually choose real names for buckets, but for right now,</p>
<p><span style="color:#808080">[00:46:06]</span> in fact, I'm going to go ahead and do that.</p>
<p><span style="color:#808080">[00:46:15]</span> Now, once I have my Dockerfile and I have my, whatever I want to run within that Docker image, and by the way, it doesn't have to be a shell script. My Dockerfile could be set up so that I can run Python files. Or whatever I want. That's the flexibility of a docker image. I could run, or for example, I could run an R script. You can run whatever you want or need to run within your docker image.</p>
<p><span style="color:#808080">[00:46:59]</span> So, once you have your docker image, what do you do with it? You can, just like you can deploy a Cloud Function to Google Cloud, you can also deploy a Cloud Run image to Google Cloud. Now the first thing that you need to do, it's actually a two step process. The first thing that you need to do is build your Dockerfile into an image.</p>
<p><span style="color:#808080">[00:47:26]</span> And you can use this command here to do something like that. Now this will Submit a new build and give that build a tag So every image needs some sort of tag to identify it In this case, I'm proposing that you tag it with generate property map tiles but you can use whatever tag you need Just like you can name your function whatever you want You can also name your images whatever you want within your project </p>
<p><span style="color:#808080">[00:47:59]</span> Now, These are all being deployed onto GCR or Google Container Repository which is a repository of containers. You can store containers there within your project.</p>
<p><span style="color:#808080">[00:48:14]</span> And then you can create a gcloud a Google Cloud Run job from your image. This tag should match whatever you named your image up here. But that will allow you to create a cloud run task. And it's essentially the same as you would do for a cloud function, except it is more general than a cloud function.</p>
<p><span style="color:#808080">[00:48:44]</span> All right, so I'm going to leave that as an exercise to you.</p>

<h2>Creating an API with Cloud Functions</h2>
<p><span style="color:#808080">[00:48:57]</span> And moving on, let's talk about what we've been doing with cloud functions and how we can use them. To be a little bit more dynamic than we have been up to this point. So, I mentioned earlier that we have been creating APIs with Cloud Functions. Those APIs don't do much. In fact, they do a lot. But, most of what they do is behind the scenes.</p>
<p><span style="color:#808080">[00:49:25]</span> So these Cloud Functions do a lot, but they're not dynamic. Each time you make a request to them, they do the same thing. So for example, these are a couple of the Cloud Functions that I created in past lectures. And each one of these has a URL that you can hit in order to kick off the Cloud Function.</p>
<p><span style="color:#808080">[00:49:49]</span> So for example, this URL here is a URL that you can make a request to. In order to start this Cloud Function. At that point, the, this is an ExtractPHL OPA Properties Function, so it's going to download the data from Open Data Philly. For the OPA Properties, and it is going to store that in Google Cloud Storage.</p>
<p><span style="color:#808080">[00:50:10]</span> Every time you run this function, every time you make a request to this URL, That's what's going to happen. It's going to download the data from the city and it's going to store it into Google Cloud Storage. </p>
<p><span style="color:#808080">[00:50:21]</span> But what if we wanted something a little bit more dynamic to happen? So, as an example, one of the tasks in the in class project is to create a table that contains the tax year assessment bins. for use in the front end. And this can be used to like create a histogram or some sort of distribution chart of the values of properties in the city. Now as this stands,</p>
<p><span style="color:#808080">[00:51:00]</span> what you're tasked with doing is calculating these bins once And then Having the front end use that same information each time it displays the prices of properties. But, you could imagine a case where instead of grabbing a static file from Google Cloud Storage, the front end application could make a request directly to a cloud function that contained the minimum and maximum values that it wanted to see a distribution of values for.</p>
<p><span style="color:#808080">[00:51:40]</span> So say it wanted to see the distribution of properties between 250 or 25, 000 and 125, 000 dollars. Then you can make a request to this function which would at that point make a live query to BigQuery in the way that we saw here.</p>
<p><span style="color:#808080">[00:52:13]</span> That function could then </p>
<p><span style="color:#808080">[00:52:17]</span> compile the distribution of values directly in the function and then instead of just returning a string success it could return a JSON object. So, for example, you don't have to just return a simple string here. You can return json. dumps and then put some actual useful data here.</p>
<p><span style="color:#808080">[00:52:46]</span> It doesn't just have to be a status message. It can be any values that you want. Obviously, 0, 1, 2, 3,</p>
<p><span style="color:#808080">[00:52:59]</span> 4, 5 is not a useful return value, but The point is that you can return data from these cloud functions that can be used directly in a front end application. </p>
<p><span style="color:#808080">[00:53:18]</span> Moreover, if there is some operation that can't be run in a cloud function that you need more, a more sophisticated set of infrastructure for, you can take your function and wrap it up in a cloud run container, and then do whatever you need to in a cloud run container.</p>
<p><span style="color:#808080">[00:53:45]</span> So this is how people get things like Django applications, or Flask applications, or Express applications onto Google Cloud. They will create a Dockerfile, essentially, to install the packages that they need for that application. And then, instead of saying, run a certain script, They might say something like Manage. py run server, if it were a Django application, for example, or npm start, if it were an express application. So, Functions framework in Python is actually based on a web framework called Flask. So any functions, any cloud functions that you write with Functions Framework, you can very easily port to a Flask application and wrap it up in Google Cloud Run.</p>
<p><span style="color:#808080">[00:54:44]</span> Any Functions Framework code that you write in Node, you can very easily port to an ExpressJS application and wrap it up in a Cloud Run Docker file. So you can get very powerful with these things very quickly.</p>
<p><span style="color:#808080">[00:55:04]</span> just based on the building blocks that we've seen thus far.</p>

</body>
</html>
